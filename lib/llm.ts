// LLM integration wrapper for local-first SaaS boilerplate
// Provides a unified interface for various LLM providers

export interface LLMJob {
  id: string
  prompt: string
  status: 'pending' | 'processing' | 'completed' | 'failed'
  output?: string
  error?: string
  model?: string
  parameters?: Record<string, any>
  started_at?: string
  completed_at?: string
  tokens_used?: number
  cost?: number
  created: string
  updated: string
}

export interface LLMProvider {
  name: string
  models: string[]
  submitJob: (prompt: string, parameters?: Record<string, any>) => Promise<string>
  getJob: (jobId: string) => Promise<LLMJob>
  listJobs: (limit?: number, offset?: number) => Promise<LLMJob[]>
}

export interface LLMConfig {
  provider: 'openai' | 'anthropic' | 'ollama' | 'mock'
  apiKey?: string
  baseUrl?: string
  defaultModel?: string
  maxTokens?: number
  temperature?: number
}

// Mock LLM provider for development and testing
class MockLLMProvider implements LLMProvider {
  name = 'mock'
  models = ['mock-model', 'mock-gpt', 'mock-claude']

  private jobs: Map<string, LLMJob> = new Map()

  async submitJob(prompt: string, parameters?: Record<string, any>): Promise<string> {
    const jobId = `mock_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`

    const job: LLMJob = {
      id: jobId,
      prompt,
      status: 'processing',
      model: parameters?.model || 'mock-model',
      parameters,
      created: new Date().toISOString(),
      updated: new Date().toISOString(),
    }

    this.jobs.set(jobId, job)

    // Simulate processing
    setTimeout(() => {
      const existingJob = this.jobs.get(jobId)
      if (existingJob) {
        existingJob.status = 'completed'
        existingJob.output = `This is a mock response to: "${prompt}". In a real implementation, this would be generated by an actual LLM.`
        existingJob.completed_at = new Date().toISOString()
        existingJob.tokens_used = Math.floor(prompt.length / 4) + 50
        existingJob.cost = (existingJob.tokens_used || 0) * 0.0001
        existingJob.updated = new Date().toISOString()
        this.jobs.set(jobId, existingJob)
      }
    }, 2000 + Math.random() * 3000) // 2-5 second delay

    return jobId
  }

  async getJob(jobId: string): Promise<LLMJob> {
    const job = this.jobs.get(jobId)
    if (!job) {
      throw new Error(`Job ${jobId} not found`)
    }
    return job
  }

  async listJobs(limit = 20, offset = 0): Promise<LLMJob[]> {
    const jobs = Array.from(this.jobs.values())
    return jobs
      .sort((a, b) => new Date(b.created).getTime() - new Date(a.created).getTime())
      .slice(offset, offset + limit)
  }
}

// OpenAI provider
class OpenAIProvider implements LLMProvider {
  name = 'openai'
  models = ['gpt-4', 'gpt-4-turbo-preview', 'gpt-3.5-turbo']

  constructor(private config: LLMConfig) {}

  async submitJob(prompt: string, parameters?: Record<string, any>): Promise<string> {
    const response = await fetch(`${this.config.baseUrl || 'https://api.openai.com'}/v1/chat/completions`, {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${this.config.apiKey}`,
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model: parameters?.model || this.config.defaultModel || 'gpt-3.5-turbo',
        messages: [{ role: 'user', content: prompt }],
        max_tokens: parameters?.max_tokens || this.config.maxTokens || 1000,
        temperature: parameters?.temperature || this.config.temperature || 0.7,
      }),
    })

    if (!response.ok) {
      throw new Error(`OpenAI API error: ${response.statusText}`)
    }

    const data = await response.json()
    const jobId = `openai_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`

    // In a real implementation, you'd store this in your database
    // For now, we'll return the job ID and handle the response
    return jobId
  }

  async getJob(jobId: string): Promise<LLMJob> {
    // Implementation would retrieve from database
    throw new Error('Not implemented - requires database integration')
  }

  async listJobs(limit = 20, offset = 0): Promise<LLMJob[]> {
    // Implementation would retrieve from database
    throw new Error('Not implemented - requires database integration')
  }
}

// Ollama provider for local LLMs
class OllamaProvider implements LLMProvider {
  name = 'ollama'
  models: string[] = []

  constructor(private config: LLMConfig) {
    this.fetchModels()
  }

  private async fetchModels() {
    try {
      const response = await fetch(`${this.config.baseUrl || 'http://localhost:11434'}/api/tags`)
      if (response.ok) {
        const data = await response.json()
        this.models = data.models?.map((m: any) => m.name) || []
      }
    } catch (error) {
      console.warn('Failed to fetch Ollama models:', error)
    }
  }

  async submitJob(prompt: string, parameters?: Record<string, any>): Promise<string> {
    const response = await fetch(`${this.config.baseUrl || 'http://localhost:11434'}/api/generate`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model: parameters?.model || this.config.defaultModel || 'llama2',
        prompt,
        stream: false,
        options: {
          temperature: parameters?.temperature || this.config.temperature || 0.7,
          num_predict: parameters?.max_tokens || this.config.maxTokens || 1000,
        },
      }),
    })

    if (!response.ok) {
      throw new Error(`Ollama API error: ${response.statusText}`)
    }

    const data = await response.json()
    const jobId = `ollama_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`

    return jobId
  }

  async getJob(jobId: string): Promise<LLMJob> {
    throw new Error('Not implemented - requires database integration')
  }

  async listJobs(limit = 20, offset = 0): Promise<LLMJob[]> {
    throw new Error('Not implemented - requires database integration')
  }
}

// Main LLM service class
export class LLMService {
  private provider: LLMProvider
  private config: LLMConfig

  constructor(config: LLMConfig) {
    this.config = config

    switch (config.provider) {
      case 'openai':
        this.provider = new OpenAIProvider(config)
        break
      case 'ollama':
        this.provider = new OllamaProvider(config)
        break
      case 'mock':
      default:
        this.provider = new MockLLMProvider()
        break
    }
  }

  async submitJob(prompt: string, metadata?: {
    model?: string
    maxTokens?: number
    temperature?: number
    parameters?: Record<string, any>
  }): Promise<string> {
    const parameters = {
      model: metadata?.model,
      max_tokens: metadata?.maxTokens,
      temperature: metadata?.temperature,
      ...metadata?.parameters,
    }

    return await this.provider.submitJob(prompt, parameters)
  }

  async getJob(jobId: string): Promise<LLMJob> {
    return await this.provider.getJob(jobId)
  }

  async listJobs(limit = 20, offset = 0): Promise<LLMJob[]> {
    return await this.provider.listJobs(limit, offset)
  }

  getAvailableModels(): string[] {
    return this.provider.models
  }

  getProviderName(): string {
    return this.provider.name
  }

  // Utility method to estimate token count
  estimateTokens(text: string): number {
    // Rough estimation: ~4 characters per token
    return Math.ceil(text.length / 4)
  }

  // Utility method to estimate cost
  estimateCost(tokens: number, model?: string): number {
    // Rough estimation based on common pricing
    const rates: Record<string, number> = {
      'gpt-4': 0.03 / 1000, // $0.03 per 1K tokens
      'gpt-3.5-turbo': 0.002 / 1000, // $0.002 per 1K tokens
    }

    const rate = model && rates[model] ? rates[model] : 0.002 / 1000
    return tokens * rate
  }
}

// Factory function to create LLM service
export function createLLMService(config?: Partial<LLMConfig>): LLMService {
  const defaultConfig: LLMConfig = {
    provider: 'mock',
    defaultModel: 'mock-model',
    maxTokens: 1000,
    temperature: 0.7,
  }

  return new LLMService({ ...defaultConfig, ...config })
}

// Export singleton instance with mock provider for development
export const llmService = createLLMService({
  provider: 'mock',
})
